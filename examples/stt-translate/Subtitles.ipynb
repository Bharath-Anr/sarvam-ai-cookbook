{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Processing and Transcription using Sarvam AI API and VAD\n",
        "\n",
        "This notebook demonstrates how to process an audio file, detect speech segments using Voice Activity Detection (VAD), and transcribe those segments using the Sarvam AI Speech-to-Text API. The results are then saved in an SRT file format.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have the following installed:\n",
        "\n",
        "- Python 3.7 or higher\n",
        "- Required Python packages: `torch`, `numpy`, `librosa`, `soundfile`, `tqdm`, `pydub`, `requests`\n",
        "\n"
      ],
      "metadata": {
        "id": "NP_6cflBTZEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy librosa soundfile tqdm pydub requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NrPAhK2LVuO",
        "outputId": "0d1c6c3d-0846-430e-a80c-806332e0ef93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Set Up the API Endpoint and Payload**\n",
        "\n",
        "To use the Saaras API, you need an API subscription key. Follow these steps to set up your API key:\n",
        "\n",
        "1. **Obtain your API key**: If you donâ€™t have an API key, sign up on the [Sarvam AI Dashboard](https://dashboard.sarvam.ai/) to get one.\n",
        "2. **Replace the placeholder key**: In the code below, replace \"YOUR_SARVAM_AI_API_KEY\" with your actual API key."
      ],
      "metadata": {
        "id": "13qJ0ks7Wp3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SARVAM_AI_API=\"YOUR_SARVAM_AI_API_KEY\""
      ],
      "metadata": {
        "id": "NpoUThmxWuOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Configuration**\n",
        "\n",
        "Set up the configuration parameters for the audio processing and VAD."
      ],
      "metadata": {
        "id": "r5sCN2dZTya-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate = 16000  # Set the sample rate for loading audio\n",
        "vad_threshold = 0.5  # Threshold for VAD\n",
        "combine_duration = 8  # Maximum duration for combined segments\n",
        "combine_gap = 1  # Maximum gap between segments to combine"
      ],
      "metadata": {
        "id": "IT0ExtXmUCcf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load VAD Model\n",
        "\n",
        "We use the Silero VAD model from the `torch.hub` to detect speech segments in the audio.\n"
      ],
      "metadata": {
        "id": "JX1h5b-CUcZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def get_vad_probs(model, audio, sample_rate=16000):\n",
        "    audio = torch.as_tensor(audio, dtype=torch.float32)\n",
        "    window_size_samples = 512 if sample_rate == 16000 else 256\n",
        "\n",
        "    model.reset_states()\n",
        "    audio_length_samples = len(audio)\n",
        "\n",
        "    speech_probs = []\n",
        "    for current_start_sample in range(0, audio_length_samples, window_size_samples):\n",
        "        chunk = audio[current_start_sample: current_start_sample + window_size_samples]\n",
        "        if len(chunk) < window_size_samples:\n",
        "            chunk = torch.nn.functional.pad(chunk, (0, int(window_size_samples - len(chunk))))\n",
        "        speech_prob = model(chunk, sample_rate).item()\n",
        "        speech_probs.append(speech_prob)\n",
        "\n",
        "    return speech_probs\n"
      ],
      "metadata": {
        "id": "UqKjAApNUf4L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Utterances\n",
        "\n",
        "This function extracts the start and end times of speech segments based on the VAD probabilities.\n"
      ],
      "metadata": {
        "id": "leYXyGE8Uh7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_utterances(vad_probs, threshold=0.5, frame_duration=0.032):\n",
        "    \"\"\"Extracts utterances (start and end times) based on VAD probabilities.\"\"\"\n",
        "    utterances = []\n",
        "    in_utterance = False\n",
        "    utterance_start = 0\n",
        "\n",
        "    for i, prob in enumerate(vad_probs):\n",
        "        if prob > threshold and not in_utterance:\n",
        "            in_utterance = True\n",
        "            utterance_start = i * frame_duration\n",
        "        elif prob <= threshold and in_utterance:\n",
        "            in_utterance = False\n",
        "            utterance_end = i * frame_duration\n",
        "            if utterance_end - utterance_start > 0:\n",
        "                utterances.append((utterance_start, utterance_end))\n",
        "\n",
        "    if in_utterance:\n",
        "        utterances.append((utterance_start, len(vad_probs) * frame_duration))\n",
        "\n",
        "    return utterances\n"
      ],
      "metadata": {
        "id": "ApMimO20UjPL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge Segments\n",
        "\n",
        "This function merges segments that are close to each other and within the specified duration limit.\n"
      ],
      "metadata": {
        "id": "tH0-xhhfUk-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_segments(segments, max_duration=8, max_gap=1):\n",
        "    \"\"\"Combines segments with pauses shorter than `max_gap` seconds, with total duration limit.\"\"\"\n",
        "    merged_segments = []\n",
        "    if not segments:\n",
        "        return merged_segments  # Return empty if no segments are found\n",
        "\n",
        "    current_start, current_end = segments[0]\n",
        "\n",
        "    for start, end in segments[1:]:\n",
        "        combined_duration = (end - current_start)\n",
        "\n",
        "        if (start - current_end <= max_gap) and (combined_duration <= max_duration):\n",
        "            current_end = end\n",
        "        else:\n",
        "            merged_segments.append((current_start, current_end))\n",
        "            current_start, current_end = start, end\n",
        "\n",
        "    merged_segments.append((current_start, current_end))\n",
        "    return merged_segments\n"
      ],
      "metadata": {
        "id": "C_nQOGe3Ul-7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Audio\n",
        "\n",
        "This function processes the audio file to detect speech segments using the VAD model.\n"
      ],
      "metadata": {
        "id": "MDafi2v1UnWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(audio_file):\n",
        "    vad_model, _ = torch.hub.load(\n",
        "        repo_or_dir='snakers4/silero-vad',\n",
        "        model='silero_vad',\n",
        "        force_reload=False,\n",
        "        onnx=False\n",
        "    )\n",
        "    vad_model.eval()\n",
        "\n",
        "    audio, _ = librosa.load(audio_file, sr=sample_rate)\n",
        "    speech_probs = get_vad_probs(vad_model, audio, sample_rate)\n",
        "    utterances = get_utterances(speech_probs, threshold=vad_threshold)\n",
        "\n",
        "    if not utterances:\n",
        "        print(f\"No VAD regions detected for {audio_file}.\")\n",
        "        return\n",
        "    merged_segments = merge_segments(utterances, max_duration=combine_duration, max_gap=combine_gap)\n",
        "\n",
        "    if merged_segments:\n",
        "        return merged_segments\n",
        "    else:\n",
        "        return\n"
      ],
      "metadata": {
        "id": "uZImMlOCUokQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription using Sarvam AI API\n",
        "\n",
        "### Transcribe Audio Segment\n",
        "\n",
        "This function sends an audio segment to the Sarvam AI API for transcription.\n"
      ],
      "metadata": {
        "id": "XDpHO_5dUqcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio_segment(start_time_sec, end_time_sec):\n",
        "    # Convert seconds to milliseconds for pydub\n",
        "    start_time_ms = start_time_sec * 1000\n",
        "    end_time_ms = end_time_sec * 1000\n",
        "\n",
        "    # Extract the audio segment\n",
        "    segment = audio[start_time_ms:end_time_ms]\n",
        "\n",
        "    # Export segment to an in-memory BytesIO object\n",
        "    audio_buffer = io.BytesIO()\n",
        "    segment.export(audio_buffer, format=\"wav\")\n",
        "    audio_buffer.seek(0)  # Reset buffer position to the beginning\n",
        "\n",
        "    files = {\n",
        "        'file': ('audiofile.wav', audio_buffer, 'audio/wav')\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, files=files, data=data)\n",
        "\n",
        "    if response.status_code == 200 or response.status_code == 201:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error for segment {start_time_sec}-{end_time_sec}: {response.status_code} - {response.text}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "40VjXm8AUrv6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write SRT File\n",
        "\n",
        "This function writes the transcription results into an SRT file.\n"
      ],
      "metadata": {
        "id": "vNsa79bCUs9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_srt_file(results, output_file_path):\n",
        "    \"\"\"\n",
        "    Writes the transcription results into an SRT file.\n",
        "\n",
        "    Args:\n",
        "        results (list): List of dictionaries containing 'start_time', 'end_time', and 'transcript'.\n",
        "        output_file_path (str): Path to save the SRT file.\n",
        "    \"\"\"\n",
        "    def format_timestamp(seconds):\n",
        "        \"\"\"Converts seconds to SRT timestamp format: hh:mm:ss,ms\"\"\"\n",
        "        milliseconds = int((seconds % 1) * 1000)\n",
        "        seconds = int(seconds)\n",
        "        minutes = seconds // 60\n",
        "        hours = minutes // 60\n",
        "        seconds = seconds % 60\n",
        "        minutes = minutes % 60\n",
        "        return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
        "\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
        "        for i, result in enumerate(results, start=1):\n",
        "            start_timestamp = format_timestamp(result[\"start_time\"])\n",
        "            end_timestamp = format_timestamp(result[\"end_time\"])\n",
        "            transcript = result[\"transcript\"]\n",
        "\n",
        "            # Write the SRT entry\n",
        "            srt_file.write(f\"{i}\\\\n\")\n",
        "            srt_file.write(f\"{start_timestamp} --> {end_timestamp}\\\\n\")\n",
        "            srt_file.write(f\"{transcript}\\\\n\\\\n\")\n"
      ],
      "metadata": {
        "id": "wWD810n0UuAk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Execution\n",
        "\n",
        "### Set Up API and Audio File\n",
        "\n",
        "Set up the Sarvam AI API URL, headers, and data. Also, specify the path to the audio file.\n"
      ],
      "metadata": {
        "id": "9Hei0Ni0Uvgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_url = \"https://api.sarvam.ai/speech-to-text-translate\"\n",
        "headers = {\n",
        "    \"api-subscription-key\" : \"d75d7bf3-b053-4084-ac80-c37561a35bfc\"\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"saaras:v2\",\n",
        "}\n",
        "audio_file_path = \"stevve.wav\"\n"
      ],
      "metadata": {
        "id": "La0M6yBPUyGH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Audio and Transcribe\n",
        "\n",
        "Process the audio file to detect speech segments and transcribe each segment using the Sarvam AI API.\n"
      ],
      "metadata": {
        "id": "Xh6RmPfiUzlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestamps = process_audio(audio_file_path)\n",
        "audio = AudioSegment.from_file(audio_file_path)\n",
        "\n",
        "results = []\n",
        "\n",
        "for start, end in timestamps:\n",
        "    transcription = transcribe_audio_segment(start, end)\n",
        "    if transcription is not None:\n",
        "        results.append({\n",
        "            \"start_time\": start,\n",
        "            \"end_time\": end,\n",
        "            \"transcript\": transcription[\"transcript\"]\n",
        "        })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wmGtrVKU0ww",
        "outputId": "3c725536-214e-4a2e-9211-5ea03504322c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Results to SRT File\n",
        "\n",
        "Finally, save the transcription results to an SRT file.\n"
      ],
      "metadata": {
        "id": "YpJ-2cwlU2dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_srt_path = \"subtitles.srt\"\n",
        "write_srt_file(results, output_srt_path)\n",
        "\n",
        "print(f\"SRT file has been saved to {output_srt_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GZ4WQYeU3io",
        "outputId": "8e6fd1fa-b3d9-48fd-d39c-34e847ac3ea6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRT file has been saved to subtitles.srt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates how to process an audio file, detect speech segments using VAD, transcribe those segments using the Sarvam AI API, and save the results in an SRT file format. You can modify the configuration parameters and API settings to suit your specific needs.\n"
      ],
      "metadata": {
        "id": "HdP1ILf1U40Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "working\n"
      ],
      "metadata": {
        "id": "DVGUUA9GT30D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j55ziLZ2K8qj",
        "outputId": "e8f3b921-ac02-44fb-d1f6-e8512aaffd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRT file has been saved to subtitles.srt\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9hdhcAGLf61"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}