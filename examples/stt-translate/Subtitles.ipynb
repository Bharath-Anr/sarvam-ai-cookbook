{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP_6cflBTZEa"
      },
      "source": [
        "# Audio Processing and Transcription using Sarvam AI API and VAD\n",
        "\n",
        "This notebook demonstrates how to process an audio file, detect speech segments using Voice Activity Detection (VAD), and transcribe those segments using the Sarvam AI Speech-to-Text API. The results are then saved in an SRT file format.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have the following installed:\n",
        "\n",
        "- Python 3.7 or higher\n",
        "- Required Python packages: `torch`, `numpy`, `librosa`, `soundfile`, `tqdm`, `pydub`, `requests`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NrPAhK2LVuO",
        "outputId": "0d1c6c3d-0846-430e-a80c-806332e0ef93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: librosa in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.2.post1)\n",
            "Requirement already satisfied: soundfile in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.67.0)\n",
            "Requirement already satisfied: pydub in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.25.1)\n",
            "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.5.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: joblib>=0.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: pycparser in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
            "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
            "Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.1/2.4 MB 2.4 MB/s eta 0:00:01\n",
            "   ----- ---------------------------------- 0.3/2.4 MB 3.2 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 0.5/2.4 MB 3.8 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 0.7/2.4 MB 3.8 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 0.9/2.4 MB 3.9 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 1.1/2.4 MB 4.0 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 1.3/2.4 MB 4.0 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.5/2.4 MB 4.1 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 1.8/2.4 MB 4.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 1.9/2.4 MB 4.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.2/2.4 MB 4.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.4/2.4 MB 4.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 4.1 MB/s eta 0:00:00\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-2.5.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install torch numpy librosa soundfile tqdm pydub requests torchaudio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import required packages\n",
        "\n",
        "To use the required libraries in your Python script, you can import them in a single line as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torchaudio\n",
        "import io\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from pydub import AudioSegment\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13qJ0ks7Wp3P"
      },
      "source": [
        "### **Set Up the API Endpoint and Payload**\n",
        "\n",
        "To use the Saaras API, you need an API subscription key. Follow these steps to set up your API key:\n",
        "\n",
        "1. **Obtain your API key**: If you don’t have an API key, sign up on the [Sarvam AI Dashboard](https://dashboard.sarvam.ai/) to get one.\n",
        "2. **Replace the placeholder key**: In the code below, replace \"YOUR_SARVAM_AI_API_KEY\" with your actual API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NpoUThmxWuOd"
      },
      "outputs": [],
      "source": [
        "SARVAM_AI_API=\"YOUR_SARVAM_AI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5sCN2dZTya-"
      },
      "source": [
        "## **Configuration**\n",
        "\n",
        "Set up the configuration parameters for the audio processing and VAD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IT0ExtXmUCcf"
      },
      "outputs": [],
      "source": [
        "sample_rate = 16000  # Set the sample rate for loading audio\n",
        "vad_threshold = 0.5  # Threshold for VAD\n",
        "combine_duration = 8  # Maximum duration for combined segments\n",
        "combine_gap = 1  # Maximum gap between segments to combine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX1h5b-CUcZx"
      },
      "source": [
        "### Load VAD Model\n",
        "\n",
        "We use the Silero VAD model from the `torch.hub` to detect speech segments in the audio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UqKjAApNUf4L"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def get_vad_probs(model, audio, sample_rate=16000):\n",
        "    audio = torch.as_tensor(audio, dtype=torch.float32)\n",
        "    window_size_samples = 512 if sample_rate == 16000 else 256\n",
        "\n",
        "    model.reset_states()\n",
        "    audio_length_samples = len(audio)\n",
        "\n",
        "    speech_probs = []\n",
        "    for current_start_sample in range(0, audio_length_samples, window_size_samples):\n",
        "        chunk = audio[current_start_sample: current_start_sample + window_size_samples]\n",
        "        if len(chunk) < window_size_samples:\n",
        "            chunk = torch.nn.functional.pad(chunk, (0, int(window_size_samples - len(chunk))))\n",
        "        speech_prob = model(chunk, sample_rate).item()\n",
        "        speech_probs.append(speech_prob)\n",
        "\n",
        "    return speech_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leYXyGE8Uh7p"
      },
      "source": [
        "### Extract Utterances\n",
        "\n",
        "This function extracts the start and end times of speech segments based on the VAD probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ApMimO20UjPL"
      },
      "outputs": [],
      "source": [
        "def get_utterances(vad_probs, threshold=0.5, frame_duration=0.032):\n",
        "    \"\"\"Extracts utterances (start and end times) based on VAD probabilities.\"\"\"\n",
        "    utterances = []\n",
        "    in_utterance = False\n",
        "    utterance_start = 0\n",
        "\n",
        "    for i, prob in enumerate(vad_probs):\n",
        "        if prob > threshold and not in_utterance:\n",
        "            in_utterance = True\n",
        "            utterance_start = i * frame_duration\n",
        "        elif prob <= threshold and in_utterance:\n",
        "            in_utterance = False\n",
        "            utterance_end = i * frame_duration\n",
        "            if utterance_end - utterance_start > 0:\n",
        "                utterances.append((utterance_start, utterance_end))\n",
        "\n",
        "    if in_utterance:\n",
        "        utterances.append((utterance_start, len(vad_probs) * frame_duration))\n",
        "\n",
        "    return utterances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0-xhhfUk-q"
      },
      "source": [
        "### Merge Segments\n",
        "\n",
        "This function merges segments that are close to each other and within the specified duration limit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "C_nQOGe3Ul-7"
      },
      "outputs": [],
      "source": [
        "def merge_segments(segments, max_duration=8, max_gap=1):\n",
        "    \"\"\"Combines segments with pauses shorter than `max_gap` seconds, with total duration limit.\"\"\"\n",
        "    merged_segments = []\n",
        "    if not segments:\n",
        "        return merged_segments  # Return empty if no segments are found\n",
        "\n",
        "    current_start, current_end = segments[0]\n",
        "\n",
        "    for start, end in segments[1:]:\n",
        "        combined_duration = (end - current_start)\n",
        "\n",
        "        if (start - current_end <= max_gap) and (combined_duration <= max_duration):\n",
        "            current_end = end\n",
        "        else:\n",
        "            merged_segments.append((current_start, current_end))\n",
        "            current_start, current_end = start, end\n",
        "\n",
        "    merged_segments.append((current_start, current_end))\n",
        "    return merged_segments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDafi2v1UnWl"
      },
      "source": [
        "### Process Audio\n",
        "\n",
        "This function processes the audio file to detect speech segments using the VAD model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uZImMlOCUokQ"
      },
      "outputs": [],
      "source": [
        "def process_audio(audio_file):\n",
        "    vad_model, _ = torch.hub.load(\n",
        "        repo_or_dir='snakers4/silero-vad',\n",
        "        model='silero_vad',\n",
        "        force_reload=False,\n",
        "        onnx=False\n",
        "    )\n",
        "    vad_model.eval()\n",
        "\n",
        "    audio, _ = librosa.load(audio_file, sr=sample_rate)\n",
        "    speech_probs = get_vad_probs(vad_model, audio, sample_rate)\n",
        "    utterances = get_utterances(speech_probs, threshold=vad_threshold)\n",
        "\n",
        "    if not utterances:\n",
        "        print(f\"No VAD regions detected for {audio_file}.\")\n",
        "        return\n",
        "    merged_segments = merge_segments(utterances, max_duration=combine_duration, max_gap=combine_gap)\n",
        "\n",
        "    if merged_segments:\n",
        "        return merged_segments\n",
        "    else:\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDpHO_5dUqcl"
      },
      "source": [
        "## Transcription using Sarvam AI API\n",
        "\n",
        "### Transcribe Audio Segment\n",
        "\n",
        "This function sends an audio segment to the Sarvam AI API for transcription.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "40VjXm8AUrv6"
      },
      "outputs": [],
      "source": [
        "def transcribe_audio_segment(start_time_sec, end_time_sec):\n",
        "    # Convert seconds to milliseconds for pydub\n",
        "    start_time_ms = start_time_sec * 1000\n",
        "    end_time_ms = end_time_sec * 1000\n",
        "\n",
        "    # Extract the audio segment\n",
        "    segment = audio[start_time_ms:end_time_ms]\n",
        "\n",
        "    # Export segment to an in-memory BytesIO object\n",
        "    audio_buffer = io.BytesIO()\n",
        "    segment.export(audio_buffer, format=\"wav\")\n",
        "    audio_buffer.seek(0)  # Reset buffer position to the beginning\n",
        "\n",
        "    files = {\n",
        "        'file': ('audiofile.wav', audio_buffer, 'audio/wav')\n",
        "    }\n",
        "\n",
        "    response = requests.post(api_url, headers=headers, files=files, data=data)\n",
        "\n",
        "    if response.status_code == 200 or response.status_code == 201:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error for segment {start_time_sec}-{end_time_sec}: {response.status_code} - {response.text}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNsa79bCUs9c"
      },
      "source": [
        "### Write SRT File\n",
        "\n",
        "This function writes the transcription results into an SRT file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wWD810n0UuAk"
      },
      "outputs": [],
      "source": [
        "def write_srt_file(results, output_file_path):\n",
        "    \"\"\"\n",
        "    Writes the transcription results into an SRT file.\n",
        "\n",
        "    Args:\n",
        "        results (list): List of dictionaries containing 'start_time', 'end_time', and 'transcript'.\n",
        "        output_file_path (str): Path to save the SRT file.\n",
        "    \"\"\"\n",
        "    def format_timestamp(seconds):\n",
        "        \"\"\"Converts seconds to SRT timestamp format: hh:mm:ss,ms\"\"\"\n",
        "        milliseconds = int((seconds % 1) * 1000)\n",
        "        seconds = int(seconds)\n",
        "        minutes = seconds // 60\n",
        "        hours = minutes // 60\n",
        "        seconds = seconds % 60\n",
        "        minutes = minutes % 60\n",
        "        return f\"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}\"\n",
        "\n",
        "    with open(output_file_path, \"w\", encoding=\"utf-8\") as srt_file:\n",
        "        for i, result in enumerate(results, start=1):\n",
        "            start_timestamp = format_timestamp(result[\"start_time\"])\n",
        "            end_timestamp = format_timestamp(result[\"end_time\"])\n",
        "            transcript = result[\"transcript\"]\n",
        "\n",
        "            # Write the SRT entry\n",
        "            srt_file.write(f\"{i}\\\\n\")\n",
        "            srt_file.write(f\"{start_timestamp} --> {end_timestamp}\\\\n\")\n",
        "            srt_file.write(f\"{transcript}\\\\n\\\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hei0Ni0Uvgv"
      },
      "source": [
        "## Main Execution\n",
        "\n",
        "### Set Up API and Audio File\n",
        "\n",
        "Set up the Sarvam AI API URL, headers, and data. Also, specify the path to the audio file.\n",
        "\n",
        "Note:- Please make sure to upload your audio (.wav) file, once uploaded please provide the correct audio_file_path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "La0M6yBPUyGH"
      },
      "outputs": [],
      "source": [
        "api_url = \"https://api.sarvam.ai/speech-to-text-translate\"\n",
        "headers = {\n",
        "    \"api-subscription-key\" :SARVAM_AI_API\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"saaras:v2\",\n",
        "}\n",
        "audio_file_path = \"Your_Audio_File_Path\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh6RmPfiUzlS"
      },
      "source": [
        "### Process Audio and Transcribe\n",
        "\n",
        "Process the audio file to detect speech segments and transcribe each segment using the Sarvam AI API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wmGtrVKU0ww",
        "outputId": "3c725536-214e-4a2e-9211-5ea03504322c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Hp/.cache\\torch\\hub\\snakers4_silero-vad_master\n",
            "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_2444\\3362829905.py:10: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, _ = librosa.load(audio_file, sr=sample_rate)\n",
            "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'stevve.wav'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:209\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\soundfile.py:1265\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1264\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
            "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'stevve.wav': System error.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m timestamps \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m audio \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mfrom_file(audio_file_path)\n\u001b[0;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
            "Cell \u001b[1;32mIn[26], line 10\u001b[0m, in \u001b[0;36mprocess_audio\u001b[1;34m(audio_file)\u001b[0m\n\u001b[0;32m      2\u001b[0m vad_model, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m      3\u001b[0m     repo_or_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnakers4/silero-vad\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilero_vad\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     force_reload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     onnx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m vad_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 10\u001b[0m audio, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m speech_probs \u001b[38;5;241m=\u001b[39m get_vad_probs(vad_model, audio, sample_rate)\n\u001b[0;32m     12\u001b[0m utterances \u001b[38;5;241m=\u001b[39m get_utterances(speech_probs, threshold\u001b[38;5;241m=\u001b[39mvad_threshold)\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:240\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    237\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    243\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stevve.wav'"
          ]
        }
      ],
      "source": [
        "\n",
        "timestamps = process_audio(audio_file_path)\n",
        "audio = AudioSegment.from_file(audio_file_path)\n",
        "\n",
        "results = []\n",
        "\n",
        "for start, end in timestamps:\n",
        "    transcription = transcribe_audio_segment(start, end)\n",
        "    if transcription is not None:\n",
        "        results.append({\n",
        "            \"start_time\": start,\n",
        "            \"end_time\": end,\n",
        "            \"transcript\": transcription[\"transcript\"]\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpJ-2cwlU2dc"
      },
      "source": [
        "### Save Results to SRT File\n",
        "\n",
        "Finally, save the transcription results to an SRT file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GZ4WQYeU3io",
        "outputId": "8e6fd1fa-b3d9-48fd-d39c-34e847ac3ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SRT file has been saved to subtitles.srt\n"
          ]
        }
      ],
      "source": [
        "output_srt_path = \"subtitles.srt\"\n",
        "write_srt_file(results, output_srt_path)\n",
        "\n",
        "print(f\"SRT file has been saved to {output_srt_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdP1ILf1U40Y"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates how to process an audio file, detect speech segments using VAD, transcribe those segments using the Sarvam AI API, and save the results in an SRT file format. You can modify the configuration parameters and API settings to suit your specific needs.\n",
        "\n",
        "\n",
        "\n",
        "### **Additional Resources**\n",
        "\n",
        "For more details, refer to the official **Saaras API documentation** and join the community for support:\n",
        "\n",
        "- **Documentation**: [docs.sarvam.ai](https://docs.sarvam.ai/)\n",
        "- **Community**: [Join the Discord Community](https://discord.gg/hTuVuPNF)\n",
        "\n",
        "### **Notes:**\n",
        "\n",
        "**File Format:** Ensure the file is in .wav format and has a sample rate of 16kHz.\n",
        "\n",
        "**API Key:** Double-check that the SARVAM_API_KEY is correctly set.\n",
        "\n",
        "**Error Handling:** If transcription fails, the error message and response content will be displayed for debugging.\n",
        "\n",
        "**Keep Building!** 🚀\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
